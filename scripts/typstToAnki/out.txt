"What are most HPC machines";"Collection of many smaller machines"
"Moore's Law";"For a given chip size, the number of components would double every year.

Revised to doubling every 2 years."
"Amdahl's Law";"$S = 1/((1-p) + p/n)$

Shows the relation between serial portions and parallel portions in a program."
"What are the 4 main 'stream' paradigms? and explain them.";"- SISD
  - To improve these processor need to: increase clock speed, decrease transistor size, or increase complexity
  - Very Expensive
- SIMD
  - One instruction is applied to multiple pieces fo data in the same clock cycle
  - Common implementations are SSE or AVX
  - Also seen in GPU computing
- MISD
  - One piece of data is operated upon by multiple instructions within the same clock cycle.
  - Not often used in HPC
- MIMD
  - Multiple instructions can be applied to multiple pieces of data in the same clock cycle.
  - Commonly used in HPC"
"What are the 4 levels of parallelism?";"1. Instruction
  - Pipelining
  - Vectorisation
2. Thread
  - OpenMP
3. Process
  - MPI
4. Job
  - Independent codes
  - Queue-based clusters"
"What are the main ways we can make a super computer faster?";"Improve computer Architecture

Improve interconnects between compute nodes

Improve compiler optimisations

Improve the algorithm used

Allow for multiple jobs and resources to be used at the same time"
"How can computer architecture change to improve performance?";"Increase clock speed

Smaller/more transistors

More cores

Multiple processors.

Faster interconnect between cores, cache, and main memory.

Use of GPUs"
"Common interconnect types in HPC";"100 Gigabit Ethernet
- New
- Fibre optics

Infiniband
- 80GB/s
- Older and more common
- Fibre optics"
"Common Networking paradigms";"#image('network-topologies.png')"
"There is useful information on compilers?";"I promise"
"How can algorithms be designed to exploit HPC compute?";"Ensure loops have no dependencies

Design so loops can be run simultaneously

Sometimes it may not be possible."
"What are the 3 main loop dependencies?";"- Flow dependency
  - a=x+y b=a+c
- Anti-dependency
  - b=a+c a=x+y
- Output dependency
  - a=2 x=a+1 a=5"
"Explain what control dependencies are?";"Conditional branching statements"
"Discuss 2 options for workload management in HPC";"SLURM
- very common
- Open source
- allocate jobs to queue and when resources are free it runs
- can define limits and priority to queues

Portable Batch System (PBS)
- proprietary
- OpenPBS is an opensource version
- Has more control over how the queue is managed
- More difficult for newer users"
"What is a thread?";"An execution context for a processor

Not a physical thing!!!

Limited to a particular NUMA region"
"Explain a way to fix control dependencies";"Remove branches and instead create a mask that can be applied to select the correct side of the condition."
"Give 3 ways in which you can assist the compiler in optimising code";"Add flags to the code

Use libraries such as OpenMP

Use specific compiler flags such as -O3"
"Explain synchronisation with respect to threads and global variables";"When threads need to access global variables they need to make sure the data isn't altered and all other threads have the same data.

Utilise Mutex locks to stop threads from accessing the data."
"Give 3 main problems with respect to synchronisation in multithreaded applications";"Race Conditions

Deadlocks

Starvation"
"What is the difference between Multithreading and Multiprocessing?";"Process records running state, a thread is just concerned with the memory stack

Threads exist within a process

Threads switch faster than processes

Threads can hold all the time, starve other threads."
"What is superthreading?";"Each clock cycle contains only one threads instructions

Switch between threads each cycle"
"What is hyperthreading?";"Each clock cycle can contain multiple threads instructions.

Often found in modern processors."
"What is OpenMP? And what thread model does it utilise";"A pragma based multithreading library available in C, C++ and Fortran.

Uses the fork-join model."
"Give the basics on how to parellelise a for loop with OpenMP";"\#pragma omp parallel for"
"What are private variables? And what are the 3 ways of defining them in OpenMP";"variables that are local to that thread

- private: not given an initial value
- firstprivate: initial variable is set to the value of the variable
- lastprivate: variables value is set to the value in the primary thread."
"What are schedules and what are the 3 main types used?";"Schedules specify how the work is split between threads

- static: workload split evenly between threads before compute
- dynamic: workload is split into equally sized chunks, threads request chunks when required
- guided: same as dynamic, but successive chunks get smaller"
"How can you synchronise OpenMP programs";"\#pragma omp critical
- Higher overhead, can block multi-line statements
\#pragma omp atomic
- Lower overhead"
"What is a reduction?";"Allows for some simple function to be applied across multiple threads in parallel, e.g. sum, max, etc.

\#pragma omp parallel for reduction(+:sharedvariable)"
"How can I get the number of threads in a program?
How can I get the current thread number?
How can I set the number of threads in a program?
How can I get a thread safe time?";"omp_get_num_threads()

omp_get_thread_num()

omp_set_num_threads(int)

omp_get_wtime()"
"What does OMP_NUM_THREADS, OMP_SCHEDULE, and OMP_PROC_BIND do?";"They are all environment variables
1. set the number of threads available to a program
2. Sets the default schedule
3. Controls if and how threads can move between cores."
"What are the 2 main things to look for in performance analysis?";"- CPU performance
- Memory Bandwidth
- Communication Bandwidth"
"Give 2 examples of performance analysis tools";"gprof

perf"
"What's MPI and why is it needed";"A library for message passing between processors.

Supercomputers consist of many processors connected in a large network, they don't share RAM/Cache, so message passing is required to share data."
"What's the basic skeleton for MPI initialisation and finalisation?";"```C
int world_size, rank
MPI_Init(Null, Null)
MPI_Comm_size(MPI_COMM_WORLD, &world_size)
MPI_COMM_RANK(MPI_COMM_WORLD, &rank)
...
MPI_FINALIZE()
```"
"What is MPI_COMM_WORLD?";"the default communicator created by MPI_Init.

We can create our own communicators if we would like."
"How would you send a message in MPI?";"```C
MPI_Send(
  void data,
  int count,
  MPI_Datatype datatype,
  int destination,
  int tag,
  MPI_Comm communicator
)
```"
"How would you recv a message in MPI?";"```C
MPI_Send(
  void data,
  int count,
  MPI_Datatype datatype,
  int source,
  int tag,
  MPI_Comm communicator,
  MPI_Status status
)
```"
"What's the main issue with MPI_Recv and MPI_Send? What is there to fix this?";"They are both blocking functions.

There exists non-blocking version of each: MPI_Isend, MPI_Irecv"
"What are the communication modes in MPI?";"- Buffered: send instructions can start at any point and may complete before a corresponding receive instruction is executed.
- Synchronous: Send instructions can start at any point, but can only complete successfully when a receive instruction is executed.
- Ready: Send can only start if the receive has been executed."
"What MPI function do you use for 1-to-all communication? and give the function signature.";"`int MPI_Bcast(void data, int count, MPI_Datatype datatype, int root, MPI_Comm comm)`"
"What MPI function do you use for data scattering? and give the function signature.";"`int MPI_Scatter(const void sendbuf, int sendcount, MPI_Datatype sendtype, vod recvbuf, int recvcount, MPI_Datatype recvtype, int roo, MPI_Comm comm)`"
"What MPI function do you use for data gathering? and give the function signature.";"`int MPI_Gather(const void sendbuf, int sendcound, MPI_Datatype sendtype, void recvbuf, int reccount, MPI_Datatype recvtype, int roo, MPI_Comm comm)`"
"What do All\_\_ communicators do?";"Gathers and Scatters. 2 main types

`MPI_Allgather` - gather than broadcast

`MPI_Alltoall` - an Allgather but different ranks receive different data"
"What are reductions? and how do we do one in MPI?";"Apply an operations over multiple ranks.

`int MPI_Reduce(const void sendbuf, void recvbuff, int count, MPI_Datatype, datatype, MPI_Op op, int root, MPI_Comm comm)`"
"What are available Operators for reducing in MPI?";"- MPI_MAX
- MPI_MIN
- MPI_SUM
- MPI_PROD
- MPI_LAND
- MPI_BAND
- MPI_LOR
- MPI_BOR
- MPI_LXOR
- MPI_BXOR
- MPI_MAXLOC
- MPI_MINLOC"
"How do you broadcast reductions?";"`MPI_Allreduce`"
"What is performance portability?";"'A measurement of an application's performance efficiency for a given problem that can be executed correctly on all platforms in a given set.'

$
cal(p)(a, e, H) = cases((|H|)/(sum_(i in H)1/(e_i(a, p)))quad&'if i is supported' forall i in H, 0 quad& 'otherwise')
$"
"Explain the pros and cons of different packet sizes in inter process communication";"smaller packets:
- Copying data from memory to the network card can be interspersed
- Packets can be made available more quickly

larger packets:
- Ratio of data to packet header is better
- Cost of copying memory vs the injection rate of the messages to the network"
"What is domain decomposition?";"The act of splitting the domain of a problem into smaller chunks for to spread between different processors."
"What are halo's with respect to mesh decomposition?";"Halo's are the overlapping sections of data between neighboring ranks/processes."
"Assuming a grid of size NNZ points, using 1D decomposition with P processors along the y-axis:

What size subgrid is each processor responsible for?

What is the computation time for each subgrid?

What is the performance model for the execution time?";"Subgrid size is NN/PZ

Computation time is $t_c times N times N/P times Z$

Performance model is $T_p = t_c times N times N/P times Z + 2(t_s + 2t_w N Z)$"
"A performance model is given by $T_p = t_c times N times N/P times Z + 2(t_s + 2t_w N Z)$:

What happens as $P arrow infinity?$

Why is this not necessarily a good thing?";"Execution time will decrease.

Since the number of processors doesn't affect communication time, this won't change as P decreases. Since computation time decreases with increasing P, more and more of the execution time will be due to the communication overhead, so the program will be less efficient."
"Surface-to-Volume ratio (SVR) is a way to compare decomposition methods. What do the surface and volume affect, and what is a better SVR?";"Surface affects how much communication occurs, volume affects how much computation occurs (per processor).

The lower the SVR the better - it means lower proportion of communication time in the total execution time."
"What is Iso-efficiency?";"How the amount of computation performed (N) must scale with processor number (P) to keep parallel efficiency (E) constant."
"How do you calculate an algorithm's iso-efficiency function? What kind of function gives more scalability?";"The iso-efficiency function is N over P.

Functions with lower orders of P (e.g. N = O(P)) give higher scalability."
"What is Amdahl's Law? Give 2 limitations";"Let the serial fraction of a program be $s$, if we make the remaining part $n$ times faster by running it on $n$ processors, then speedup is:

$S(n) = frac(n, (1-s) + n s) <= frac(1,s)$

Limitations are that it can only tell the upper bound of the speedup for a particular algorithm, & it can't tell if other algorithms with greater parallelism exist for the problem."
"What are the limitations of asymptotic analysis?";"You ignore the lower-order terms: given an algorithm with time complexity O(nlogn), the actual time complexity could be 10n + nlogn. For small n, the 10n would dominate.

You can only tell the order of the execution time of a program, not its actual execution time. It may be the case that at certain values of n one algorithm performs better than another, even if the other is better asymptotically."
"How much more powerful is a GPU compared to a multicore CPU, and what is the reason for the performance gap?";"About 100x more powerful.

Reason is the differences in design."
"What are the design motivations for a CPU?";"To optimize the performance of sequential code.

Latency-oriented $arrow$ minimise the average instruction latency in sequential code."
"Name some of the key features of a CPU. What is the cost of these features?";"Complicated CU, which can execute instructions out-of-order or in parallel, may perform branch prediction, and can allow for data forwarding.

Large cache to reduce data access latencies.

Powerful ALU.

Cost is increased chip area & power."
"What is the design goal for GPUs?";"Throughput-oriented $arrow$ maximise the total throughput of a large number of threads, allowing individual threads to take a longer time."
"Name the key features of a GPU, and how they're different to those on a CPU.";"Most of the chip area is dedicated to FLOP calculations. Each calculation is simple, so we have a simple CU, and simple ALUs.

Calculation is deemed more important than cache, so cache size is reduced, allowing longer latencies for data access."
"What are the two parts of a CUDA program? Where do they execute?";"Host code runs on the CPU

Device code runs on the GPU."
"What are kernel functions, and how can they be identified?";"Functions which run on the GPU. They're marked with CUDA keywords."
"What is the GPU execution model?";"Start with host execution.

When a kernel function is called it's executed by a large number of GPU threads (called the grid).

When all threads of a kernel complete, the corresponding grid terminates.

Execution continues on the host until another kernel is called."
"When and how is execution configuration specified?";"Specified when invoking a kernel function.

Syntax is:

`dim3 grid(3,2,4), block(128,1,1)`

`vecAdd <<< grid, block >>> (A,B,C)`

where A,B,C are the parameters passed into `vecAdd()`"
"What is the difference in compilation between host & device code?";"Host code is compiled with a standard C compiler.

Device code is compiled to PTX code, then into an executable."
"Why can't a GPU use the same timing methods as those used for CPUs? Explain.";"There's asynchronous executions between the CPU & GPU.

After the CPU launches the kernel functions:

It places the command of running kernel function to a command queue maintained by CUDA.

CUDA runtime runs the commands at its own pace.

CPU continues to perform subsequent tasks after the kernel functions in the host code.

CPU doesn't wait for the GPU to complete the kernel function."
"Name two methods that can be used to synchronise the CPU & GPU. How do they work?";"`cudaDeviceSynchronise()`: CPU waits until all commands in the CUDA command queue have completed.

`cudaEventSynchronise(event_object)`: CPU waits until the specified event has completed."
"What is parallel IO?";"Multiple processes of a parallel program access different parts of a common file at the same time."
"An example of parallel IO is all processes sending their data to rank 0, which then writes to the file. What are the pros and cons of this approach?";"Pros: IO system only deals with IO from one process, doesn't need specialised IO libraries, results are in a single file (so easy to manage).

Cons: single point of failure, single node bottleneck, poor performance & scalability."
"An example of parallel IO is having each process write to a separate file. What are the pros and cons of this approach?";"Pros: parallel and high performance.

Cons: lots of small files to manage, if the number of processors changes how do you read back, doesn't interpolate well."
"An example of parallel IO is having multiple processes accessing data from a common file. What are the pros and cons of this approach?";"Pros: simultaneous IO from any number of processes, excellent performance & scalability, results stored in a single file, interpolates well, maps well to collective operations.

Cons: requires a more complex IO library, traditional accesses block the file, requires support of simultaneous access."
"Data Sieving is an IO optimisation. Explain what it does for reads and writes. What is a problem associated with this optimisation?";"Reads: combine lots of small accesses into a larger one, reducing the number of IO operations

Writes: read the entire region first, make changes, write back.

Writing requires locking in the file system, which can lead to false sharing - 2 addresses share the same cache line, writing to one causes both to be evicted from cache."
"Collective IO is an IO optimisation. Explain what it does. What must you guarantee about each of the processes?";"Collective IO is coordinated access to storage by a group of processes.

In collective operations, the underlying IO layers know what data is being requested by each process. The entire block is read, then data is moved to their final destinations.

Have to ensure that all processes participating in the IO call the function at the same time."
"Name and explain the 3 kinds of positioning variations that can be used for data access routines.";"Individual file pointer: when a file is opened by a process, a file pointer is created & owned by the process.

Explicit offset: data access routines that accept explicit offsets

Shared file pointer: when a file is opened collectively by a group of processes, a shared file pointer is created and stored by these processes"
"Name and explain the 2 kinds of synchronism variations that can be used for data access routines.

One of them can error after the function call. Which one errors, what's the error, and how do you prevent it?";"Blocking: don't return until the IO request is completed.

Non-blocking: initiate the operation and return immediately.

Non-blocking errors if the application buffer is accessed whilst IO is ongoing. Fixed by using some variation of `MPI_Wait...()` (e.g. `MPI_Waitall()`)."
"Name and explain the 2 kinds of coordination variations that can be used for data access routines.";"Non-collective (independent): each process does IO independently

Collective: all processes have to participate when doing IO"
"At least 1 process is accessing a file. What are the 3 situations in which consistency needs to be maintained?";"A process writes data, and the same process later reads the data

A process writes data, and another process reads the data

2 process write to the data"
"In some cases, MPI can guarantee consistency. Where it can't, what are the 3 ways a user can ensure consistency?";"Making writing data to the system buffer & transferring data to storage atomic.

Close the file & reopen it to force all writes to be transferred to storage

Use `MPI_File_sync()` to transfer all writes to storage."
"What is file interoperability?";"The ability to read the data previously written to a file."
"Name & explain the 3 different data representations.";"Native: data is stored in a file exactly as it is in memory

Internal: data is stored in an implementation-dependent format.

External32: data on the storage medium is always the standard representation (big-endian IEEE)."
"What are the pros & cons of the native representation?";"Pros: data precision & IO performance

Cons: loss of transparent interoperability (can't be used in heterogeneous environments)"
"What may be necessary when using the internal representation? What is an advantage of using this representation?";"Type conversions may be necessary.

Pros: can be used in heterogeneous environments"
"What is an advantage of using External32 representation?";"Data can be recognised by other applications."
"What are the motivations for clusters?";"Clustering allows high performance CPUs alongside high-speed networking.

High-performance CPUs are more portable & scalable than custom-designed mainframe computers."
"The cluster manager is a party in a cluster. What are the other two, and what do they each do?";"Users: submit jobs & job requirements.

Administrators: describe policy restrictions.

Cluster manager: monitors the state of the cluster, schedules jobs, tracks resource usage."
"What are the 5 main activities of a cluster manager. Explain them briefly.";"Queuing: submitted jobs are held in the queue until the job is at the head of the queue, & the matching resources are available.

Scheduling: determine what time a job should execute & on which resources.

Monitoring: provide info to admins, users & the manager on the status of jobs & resources.

Resource management: handle details of starting job execution, stopping a job, cleaning up after completion/abortion, and removing/adding resources.

Accounting: collect usage data to produce usage reports, tune the scheduling policy, anticipate future resource requirements by users, and determine the area of improvement within the cluster."
"What are 2 LAN structures? Explain them briefly.";"Shared Network (Bus)
- All processors are connected to & share the bus
- Each processor listening to every message
- Requires a complex protocol to access the bus, e.g., Carrier Sense Multiple Access/Collision Detection (CSMA/CD)
- Collisions may occur, requiring back-off policies & retransmissions
- Not suitable for HPC since it performs better when communication load is low

Switched Network
- Allows point-to-point communications between the sender & receiver
- Simultaneous internal transport provides high aggregate bandwidth
- Multiple messages are sent simultaneously"
"What are the different metrics to assess cluster topology?";"Scalability: the number of switches needed to interconnect a given number of nodes (cost per node)

Degree: the number of links to/from a node (aggregate bandwidth)

Diameter: the shortest path between the furthest nodes (latency)

Bisection width: the minimum number of links that need to be cut to divide the topology into 2 independent networks of the same size ($plus.minus$ 1 node)

Bottleneck bandwidth: the smallest bandwidth on a network. A higher number means the network performs better under heavy load."
"What are the pros and cons of the crossbar topology?";"Pros: low resource contention

Cons: switch scalability is $O(n^2)$, lots of wiring so high cost."
"What is the scalability, diameter, and bisection width for a linear array?";"Scalability and diameter are both $O(n)$.

Bisection width is $O(1)$."
"What's the difference between a linear array and ring? What's the scalability, diameter, and bisection width for a ring?";"A ring is just a linear array with wrap-around between the end nodes.

Scalability is $O(n)$.

Diameter is $O(n/2)$.

Bisection width is $O(2)$"
"What's the scaling, average degree, diameter, and bisection width of a 2D mesh?";"Scaling: $O(n)$

Average degree: 4

Diameter: $O(2n^(1/2))$

Bisection width: $O(n^(1/2))$"
"What's the scaling, average degree, diameter, and bisection width of a 2D torus?";"Scaling: $O(n)$

Average degree: 4

Diameter: $O(n^(1/2))$

Bisection width: $O(2n^(1/2))$"
"How do you make a hypercube? What's the scalability, diameter, and bisection width?";"Constructed with binary encoding & iterative graph expansion.

Scalability: $K$ dimensions requires $O(2^K)$

Diameter: $O(K)$

Bisection width: $O(2^(K-1))$"
"What's the scalability, degree, diameter, and bisection width for a binary tree?";"Scalability: depth $d$ requires $O(2^(d+1) - 1)$

Degree: 3

Diameter: $O(2d)$

Bisection width: $O(1)$"
"What's the difference between a fat tree and a binary tree? What's the bisection width and degree of a fat tree?";"Branches nearer the top of the tree have more links. Add links s.t. the number of downlinks of a switch is the same as the number of uplinks.

Bisection width: 2^(d-1)

Degree: 2^(k+1)"
"Describe the store-and-forward switching technique. What's the performance model?";"- Each switch receives an entire packet before it forwards it onto the next switch
- Useful in non-dedicated environments
- Buffer size is limited, so packets will be dropped under a heavy load
- Larger in-switch latency
- Can detect errors in the packet

Performance model is $T_(c o m m)^c = T_s + (T_h + L T_w)D$"
"Describe the wormhole (cut-through) switching technique. What's the performance model?";"- Packet is divided into small 'flits' (flow control digits)
- Switch examines the first flit (header), which contains the destination address, sets up a circuit, & forwards the filt
- Any subsequent flits are forwarded as they arrive, reducing latency & buffer overhead, but means less error detection

Performance model: $T^c_(c o m m) = T_s + D(T_h + f T_w) + T_w(L - f)$"
